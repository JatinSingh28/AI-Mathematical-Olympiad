{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":7369493,"sourceType":"datasetVersion","datasetId":4281572},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":33187,"sourceType":"modelInstanceVersion","modelInstanceId":27786},{"sourceId":33534,"sourceType":"modelInstanceVersion","modelInstanceId":28071},{"sourceId":34105,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28551},{"sourceId":34107,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28553}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jatinsinghsagoi/aimo-24-finetuned-deepseek-math-inference?scriptVersionId=174283106\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-27T11:06:06.17355Z","iopub.execute_input":"2024-04-27T11:06:06.174263Z","iopub.status.idle":"2024-04-27T11:06:42.004581Z","shell.execute_reply.started":"2024-04-27T11:06:06.174229Z","shell.execute_reply":"2024-04-27T11:06:42.003274Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U /kaggle/input/peft-wheel/pytorch/version1/1/peft-0.10.0-py3-none-any.whl -qq","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:06:42.006769Z","iopub.execute_input":"2024-04-27T11:06:42.007099Z","iopub.status.idle":"2024-04-27T11:07:15.028139Z","shell.execute_reply.started":"2024-04-27T11:06:42.007068Z","shell.execute_reply":"2024-04-27T11:07:15.027004Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install transformers accelerate peft datasets bitsandbytes torch","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:15.029644Z","iopub.execute_input":"2024-04-27T11:07:15.029949Z","iopub.status.idle":"2024-04-27T11:07:15.034845Z","shell.execute_reply.started":"2024-04-27T11:07:15.029921Z","shell.execute_reply":"2024-04-27T11:07:15.03386Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# !curl https://pypi.org/simple/flask-bcrypt/","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:15.037191Z","iopub.execute_input":"2024-04-27T11:07:15.037563Z","iopub.status.idle":"2024-04-27T11:07:15.046423Z","shell.execute_reply.started":"2024-04-27T11:07:15.03753Z","shell.execute_reply":"2024-04-27T11:07:15.04539Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# !pip wheel peft","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:15.047739Z","iopub.execute_input":"2024-04-27T11:07:15.048135Z","iopub.status.idle":"2024-04-27T11:07:15.056735Z","shell.execute_reply.started":"2024-04-27T11:07:15.0481Z","shell.execute_reply":"2024-04-27T11:07:15.055815Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# !pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:15.058Z","iopub.execute_input":"2024-04-27T11:07:15.05826Z","iopub.status.idle":"2024-04-27T11:07:15.06781Z","shell.execute_reply.started":"2024-04-27T11:07:15.058238Z","shell.execute_reply":"2024-04-27T11:07:15.066816Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import torch\nimport re\nfrom peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig, \n    AutoConfig,\n    set_seed\n)\n\nset_seed(42)\n\nMODEL_PATH = \"/kaggle/input/deepseek-math\"\n# MODEL_PATH = \"/kaggle/input/metamath/pytorch/metamath-mistral-7b/1\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nconfig = AutoConfig.from_pretrained(MODEL_PATH)\nconfig.gradient_checkpointing = True\n\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL_PATH,\n#     device_map=\"sequential\",\n#     torch_dtype=\"auto\",\n#     trust_remote_code=True,\n#     quantization_config=quantization_config,\n#     config=config\n# )\n\n# PEFT_MODEL = \"/kaggle/input/deepseek-math-qlora/pytorch/version1/1\"\n# PEFT_MODEL = \"/kaggle/input/deepseek-math-qlora/pytorch/version2/1\"\nPEFT_MODEL = \"/kaggle/input/deepseek-math-qlora/pytorch/10k-data-v1/1\"\n\nconfig = PeftConfig.from_pretrained(PEFT_MODEL)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = PeftModel.from_pretrained(model, PEFT_MODEL)","metadata":{"papermill":{"duration":664.688061,"end_time":"2024-02-29T09:36:29.988515","exception":false,"start_time":"2024-02-29T09:25:25.300454","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-27T11:07:15.069023Z","iopub.execute_input":"2024-04-27T11:07:15.069371Z","iopub.status.idle":"2024-04-27T11:07:16.580659Z","shell.execute_reply.started":"2024-04-27T11:07:15.069337Z","shell.execute_reply":"2024-04-27T11:07:16.577442Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m PEFT_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/deepseek-math-qlora/pytorch/10k-data-v1/1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(PEFT_MODEL)\n\u001b[0;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[1;32m     53\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3455\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:86\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m            Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m            quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m            in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m            `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m            https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m            for more details.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m            \"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: \n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    "],"ename":"ValueError","evalue":"\n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    ","output_type":"error"}]},{"cell_type":"code","source":"generation_config = model.generation_config\ngeneration_config.max_new_tokens = 2048\ngeneration_config.temperature = 0.7\ngeneration_config.top_p = 0.7\ngeneration_config.num_return_sequences = 1\ngeneration_config.pad_token_id = tokenizer.eos_token_id\ngeneration_config.eos_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.581439Z","iopub.status.idle":"2024-04-27T11:07:16.581783Z","shell.execute_reply.started":"2024-04-27T11:07:16.581622Z","shell.execute_reply":"2024-04-27T11:07:16.581636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def solve_peft(problem:str):\n    try:\n    #     prompt = f\"Problem: {problem}\".strip()\n        prompt = f\"\"\"You will be given a mathematical problem statement in LaTex, and you will provide a precise answer, adhering to the following rules:\n        - Solve the problem step by step but keep the answer concise (max new tokens allowed 2000).\n        - Take the modulo of Final Answer with 1000.\n        - You will guarantee a correct answer, always within the range of 0 to 999.\n        - You have to respond even when unsure or when the question is not fully understood.\n        - The answer written in the \"Final Answer\" section must be concise.\n        - Do not generate unnecessary tokens.\n        Problem statement: {problem}\n        Answer:\"\"\".strip()\n        device = \"cuda\"\n        encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.inference_mode():\n            outputs = model.generate(\n              input_ids = encoding.input_ids,\n              attention_mask = encoding.attention_mask,\n              generation_config = generation_config\n          )\n\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n    except:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.582965Z","iopub.status.idle":"2024-04-27T11:07:16.583285Z","shell.execute_reply.started":"2024-04-27T11:07:16.583131Z","shell.execute_reply":"2024-04-27T11:07:16.583144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# prompt = \"Problem: A community is building a metal fence. Each fence panel is made of 3 metal sheets, and 2 metal beams. The fence is made of 10 fence panels. If each sheet is made of 10 metal rods and each metal beam is made of 4 metal rods, how many metal rods does the community need for the fence?\".strip()\n\n# device = \"cuda\"\n# encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n# with torch.inference_mode():\n#     outputs = model.generate(\n#       input_ids = encoding.input_ids,\n#       attention_mask = encoding.attention_mask,\n#       generation_config = generation_config\n#   )\n\n# print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.584564Z","iopub.status.idle":"2024-04-27T11:07:16.584867Z","shell.execute_reply.started":"2024-04-27T11:07:16.584715Z","shell.execute_reply":"2024-04-27T11:07:16.584728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.dtype","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.586166Z","iopub.status.idle":"2024-04-27T11:07:16.58647Z","shell.execute_reply.started":"2024-04-27T11:07:16.58632Z","shell.execute_reply":"2024-04-27T11:07:16.586332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_latex_to_text(latex_equation):\n    initial_eq = latex_equation\n    try:\n        # Define regex patterns and their replacements\n        patterns = {\n            r\"\\|\\| (.*?) \\|-(\\d+) \\|\": lambda match: f\"|{match.group(1)}| - {match.group(2)}\",\n            r\"\\\\frac{(\\w+)}{(\\w+)}\": lambda match: f\"{match.group(1)}/{match.group(2)}\",\n            r\"\\\\vert\\s*(.*?)\\s*\\\\vert\": lambda match: f\"|{match.group(1)}|\",\n            r\"\\\\left\\((.*?)\\s*\\+\\s*(.*?)\\\\right\\)\": lambda match: f\"({match.group(1)} + {match.group(2)})\",\n            r\"\\^2\": lambda match: \"^2\",\n            r\"\\\\left\\((.*?),\\s*(.*?)\\\\right\\)\":lambda match: f\"({match.group(1)}, {match.group(2)})\",\n            r\"\\\\\\[\\\\vert \\\\vert (.*?) \\\\vert -(\\d+) \\\\vert=\\\\frac{(\\w+)}{(\\w+)}\\\\]\":\n            lambda match: f\"|{match.group(1)}| - {match.group(2)} = {match.group(3)}/{match.group(4)}\",\n            r'\\$': r'',\n            r'\\|': r'',\n            r'\\\\': r'',\n            r'\\\\left\\\\lfloor\\s(.*?)\\\\right\\\\rfloor': r'the greatest integer less than or equal to \\1',\n            r'\\\\left\\\\lceil\\s(.*?)\\\\right\\\\rceil': r'the smallest integer greater than or equal to \\1',\n            r'\\\\\\{ (.*?)\\\\\\}': r'the fractional part of \\1',\n\n        }\n\n        # Replace each pattern in the LaTeX equation with the corresponding text\n        for pattern, replacement in patterns.items():\n            latex_equation = re.sub(pattern, replacement, latex_equation)\n\n        return latex_equation\n    except:\n        return initial_eq","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.587798Z","iopub.status.idle":"2024-04-27T11:07:16.588134Z","shell.execute_reply.started":"2024-04-27T11:07:16.587957Z","shell.execute_reply":"2024-04-27T11:07:16.587971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nPRIVATE = True\n\n# df_test = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n# df_test.head()","metadata":{"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-27T11:07:16.58922Z","iopub.status.idle":"2024-04-27T11:07:16.589545Z","shell.execute_reply.started":"2024-04-27T11:07:16.589369Z","shell.execute_reply":"2024-04-27T11:07:16.589382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-prize/train.csv\")\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.591072Z","iopub.status.idle":"2024-04-27T11:07:16.591392Z","shell.execute_reply.started":"2024-04-27T11:07:16.591235Z","shell.execute_reply":"2024-04-27T11:07:16.591249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas(desc=\"Processing...\")\n# df['problem'] = df['problem'].progress_apply(convert_latex_to_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.592406Z","iopub.status.idle":"2024-04-27T11:07:16.592749Z","shell.execute_reply.started":"2024-04-27T11:07:16.592589Z","shell.execute_reply":"2024-04-27T11:07:16.592603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test['problem'] = df_test['problem'].progress_apply(convert_latex_to_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.593664Z","iopub.status.idle":"2024-04-27T11:07:16.593964Z","shell.execute_reply.started":"2024-04-27T11:07:16.593813Z","shell.execute_reply":"2024-04-27T11:07:16.593826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def naive_parse(answer):\n    try:\n        out = []\n        start = False\n        end = False\n        for l in reversed(list(answer)):\n            if l in '0123456789' and not end:\n                start = True\n                out.append(l)\n            else:\n                if start:\n                    end = True\n\n        out = reversed(out)\n        return int(''.join(out))\n    except:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.595764Z","iopub.status.idle":"2024-04-27T11:07:16.596065Z","shell.execute_reply.started":"2024-04-27T11:07:16.595916Z","shell.execute_reply":"2024-04-27T11:07:16.595929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import transformers\n\n# pipeline = transformers.pipeline(\n#     \"text-generation\",\n#     model=model,\n#     tokenizer=tokenizer,\n#     torch_dtype='auto',\n#     device_map=\"auto\",\n# )","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.597209Z","iopub.status.idle":"2024-04-27T11:07:16.597562Z","shell.execute_reply.started":"2024-04-27T11:07:16.597365Z","shell.execute_reply":"2024-04-27T11:07:16.597379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f\"Transformers Version: {transformers.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.598982Z","iopub.status.idle":"2024-04-27T11:07:16.599305Z","shell.execute_reply.started":"2024-04-27T11:07:16.599148Z","shell.execute_reply":"2024-04-27T11:07:16.599162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.600416Z","iopub.status.idle":"2024-04-27T11:07:16.600754Z","shell.execute_reply.started":"2024-04-27T11:07:16.6006Z","shell.execute_reply":"2024-04-27T11:07:16.600614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def solve(problem: str):\n#     prompt = f\"\"\"You will be given a mathematical problem statement, and will provide a precise answer, adhering to the following rules:\n#     - Solve the problem step by step but keep the answer concise (max new tokens allowed 2048).\n#     - Take the modulo of Final Answer with 1000.\n#     - You will guarantee a correct answer, always within the range of 0 to 999.\n#     - You have to respond even when unsure or when the question is not fully understood.\n#     - The answer written in the \"Final Answer\" section must be concise.\n#     Problem statement: {problem} \"\"\"  \n    \n#     sequence = pipeline(prompt, max_new_tokens=2048)\n#     return sequence[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.602567Z","iopub.status.idle":"2024-04-27T11:07:16.603022Z","shell.execute_reply.started":"2024-04-27T11:07:16.602782Z","shell.execute_reply":"2024-04-27T11:07:16.602801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# debug = False","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.604041Z","iopub.status.idle":"2024-04-27T11:07:16.604366Z","shell.execute_reply.started":"2024-04-27T11:07:16.604208Z","shell.execute_reply":"2024-04-27T11:07:16.604222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if debug:\n#     df1 = df\n#     df = df[:2]\n# train_gen_text = df['problem'].progress_apply(solve_peft)\n# train_gen_text = list(map(convert_latex_to_text,train_gen_text))\n# train_results = list(map(naive_parse,train_gen_text))\n# print(train_results)\n\n# score = (train_results == df['answer']).sum()\n# print(\"Score on training data: \",score)\n\n# if debug:\n#     df = df1","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.606036Z","iopub.status.idle":"2024-04-27T11:07:16.606499Z","shell.execute_reply.started":"2024-04-27T11:07:16.606248Z","shell.execute_reply":"2024-04-27T11:07:16.606268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['answer']","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.607515Z","iopub.status.idle":"2024-04-27T11:07:16.607961Z","shell.execute_reply.started":"2024-04-27T11:07:16.607728Z","shell.execute_reply":"2024-04-27T11:07:16.607746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_gen_text[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.609288Z","iopub.status.idle":"2024-04-27T11:07:16.60975Z","shell.execute_reply.started":"2024-04-27T11:07:16.609512Z","shell.execute_reply":"2024-04-27T11:07:16.609532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# answer = []\n# for prob in df_test['problem']:\n#     try:\n#         ans = solve_peft(prob)\n#     except:\n#         ans = 0\n#     answer.append(ans)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.611796Z","iopub.status.idle":"2024-04-27T11:07:16.612161Z","shell.execute_reply.started":"2024-04-27T11:07:16.611984Z","shell.execute_reply":"2024-04-27T11:07:16.611999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# answer = df_test['problem'].progress_apply(solve_peft)\n# answer = list(map(convert_latex_to_text,answer))\n# df_test['answer'] = list(map(naive_parse,answer))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.613198Z","iopub.status.idle":"2024-04-27T11:07:16.613507Z","shell.execute_reply.started":"2024-04-27T11:07:16.613344Z","shell.execute_reply":"2024-04-27T11:07:16.613357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# answer[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.614358Z","iopub.status.idle":"2024-04-27T11:07:16.614695Z","shell.execute_reply.started":"2024-04-27T11:07:16.614535Z","shell.execute_reply":"2024-04-27T11:07:16.614549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.615763Z","iopub.status.idle":"2024-04-27T11:07:16.616207Z","shell.execute_reply.started":"2024-04-27T11:07:16.615968Z","shell.execute_reply":"2024-04-27T11:07:16.615995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fsolve(problem):\n    answer = solve_peft(problem)\n    answer = convert_latex_to_text(answer)\n    return naive_parse(answer)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.617738Z","iopub.status.idle":"2024-04-27T11:07:16.618235Z","shell.execute_reply.started":"2024-04-27T11:07:16.617967Z","shell.execute_reply":"2024-04-27T11:07:16.617996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.620049Z","iopub.status.idle":"2024-04-27T11:07:16.620525Z","shell.execute_reply.started":"2024-04-27T11:07:16.620267Z","shell.execute_reply":"2024-04-27T11:07:16.620286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fsolve(\"2 times 2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.621509Z","iopub.status.idle":"2024-04-27T11:07:16.621929Z","shell.execute_reply.started":"2024-04-27T11:07:16.621713Z","shell.execute_reply":"2024-04-27T11:07:16.621731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for test, sample_submission in iter_test:\n    sample_submission['answer'] = fsolve(test['problem'])\n    env.predict(sample_submission)\n    print(test)\n    print(sample_submission, '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.623018Z","iopub.status.idle":"2024-04-27T11:07:16.62349Z","shell.execute_reply.started":"2024-04-27T11:07:16.623232Z","shell.execute_reply":"2024-04-27T11:07:16.623251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.624833Z","iopub.status.idle":"2024-04-27T11:07:16.625286Z","shell.execute_reply.started":"2024-04-27T11:07:16.625058Z","shell.execute_reply":"2024-04-27T11:07:16.625076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.626322Z","iopub.status.idle":"2024-04-27T11:07:16.626656Z","shell.execute_reply.started":"2024-04-27T11:07:16.62647Z","shell.execute_reply":"2024-04-27T11:07:16.626503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:07:16.628631Z","iopub.status.idle":"2024-04-27T11:07:16.629082Z","shell.execute_reply.started":"2024-04-27T11:07:16.628843Z","shell.execute_reply":"2024-04-27T11:07:16.628863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}